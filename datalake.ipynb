{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datalake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingjerli/RandomThought/blob/master/datalake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0j6o15jK8u7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Datalake \n",
        "\n",
        "## Tier of Data\n",
        "\n",
        "### Raw Data\n",
        "\n",
        "Everything the team is collecting (including anything from other team's production database)\n",
        "\n",
        "Store in S3 like bucket storage\n",
        "\n",
        "### Sanitized Data\n",
        "\n",
        "Use for model training\n",
        "Store in S3, Databases(SQL, NoSQL, Graph ...)\n",
        "\n",
        "### Enhanced Data\n",
        "\n",
        "Ready to share.\n",
        "Serving BI tools dashboard, flask app, Tableau, Apache Superset\n",
        "\n",
        "Store in S3, Databases, indexing with ElasticSearch\n",
        "\n",
        "Use Airflow to orchestrate all ETL jobs\n",
        "    + Lot's of operator, native Kubernetes support\n",
        "\n",
        "## Workflow Management\n",
        "\n",
        "* Apache Airflow for batch type \n",
        "* Apache Kafka for streaming type\n",
        "\n",
        "Tasks are run through AWS Lambda, Azure Function or Kubernetes services.\n",
        "\n",
        "## Query Pattern\n",
        "\n",
        "* AdHoc/EDA\n",
        "    + Serverless, minimal engineering setup to query the data\n",
        "    + High latency\n",
        "    + Expensive for well defined query pattern\n",
        "    + AWS Athena/Data Lake Analytics\n",
        "\n",
        "* Recurrent bursting(ETL/Model training/Rollup Aggregations/single project) queries\n",
        "    + Cheaper than serverless solution for known query pattern\n",
        "    + High latency\n",
        "    + Engineering setup to query(need to load into specific format or spin up query engine)\n",
        "    + Snowflake/Presto/Spark/Drill/EMR/HDInsight ...\n",
        "\n",
        "* Recurrent consisting(BI tools/multiple projects)\n",
        "    + Database(depend on use cases and query pattern) \n",
        "    + Low latency\n",
        "    + SQL, Mongo, Neo4j, ...\n",
        "\n",
        "## Non-tabular data(images, videos, sound)\n",
        "\n",
        "Spark if not fit within on machine, else python.\n",
        "\n",
        "## ML Workflow\n",
        "\n",
        "SageMaker or Azure ML Studio, or JupyterHub (python, spark, julia, R kernels) on Kubernetes for different instance type.\n",
        "\n",
        "## Data Version\n",
        "\n",
        "* Airflow dags and dockerfiles (for Kubernetes Executor) need to be source control.\n",
        "* At raw data level, \n",
        "\t+ write-only ingestion with timestamp\n",
        "\t+ maintaining a latest view of the datalake (don't use s3 versioning your Athena cannot query older version)\n",
        "\t+ sanitized and enhanced data can be reproduce at a desire timesamp with timestamp filter query\n",
        "* At sanitized and enhanced level,\n",
        "    + Use S3 version and Database snapshot.\n",
        "\n",
        "## Monitoring\n",
        "\n",
        "### Operationa\n",
        "\n",
        "* Airflow retries or fails trigger Email, PagerDuty, Jira or Twilio.\n",
        "* Output Airflow job/task statistics to Grafana like dashboard for management level.\n",
        "\n",
        "### Quality\n",
        "\n",
        "* `great_expectations` for unittest like before promoting data to production(only at enhanced level).\n",
        "* Timeseries forecasting on table metadata/aggregation values and report anomaly. \n",
        "    + My work before, scalable timeseries forcasting and using z-score and autocorrelation lag to detect anomaly, also collecting user feedback as supervised learning problem to avoid false alarm.\n",
        "\n",
        "## Discovery\n",
        "\n",
        "* Elasticsearch index\n",
        "* Metadata(tagging, labeling) collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HgyZTFJv-ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}